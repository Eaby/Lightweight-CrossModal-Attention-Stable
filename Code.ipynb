{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51810235-f0c4-45fc-a4ef-3ea3822c5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU Memory Management and cache clear Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef93c77b-33ed-4877-aa60-30b7506dfbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX 2000 Ada Generation\n",
      "VRAM: 16.7 GB\n",
      "Optimal batch size: 32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "class GPUMemoryManager:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear GPU cache to free memory\"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Monitor GPU memory usage\"\"\"\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        cached = torch.cuda.memory_reserved() / 1e9\n",
    "        return f\"Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB\"\n",
    "    \n",
    "    def optimize_batch_size(self, base_batch_size=32):\n",
    "        \"\"\"Automatically determine optimal batch size\"\"\"\n",
    "        try:\n",
    "            # Test with base batch size\n",
    "            dummy_input = torch.randn(base_batch_size, 3, 224, 224).cuda()\n",
    "            del dummy_input\n",
    "            self.clear_cache()\n",
    "            return base_batch_size\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                return self.optimize_batch_size(base_batch_size // 2)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# Usage\n",
    "gpu_manager = GPUMemoryManager()\n",
    "optimal_batch_size = gpu_manager.optimize_batch_size()\n",
    "print(f\"Optimal batch size: {optimal_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c09e6-9d9e-4a92-8e43-651a4fccc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory-Efficient Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0b1f8f-e0fb-468c-97e9-fc3c6f26b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import gc\n",
    "\n",
    "# Config for your RTX 2000 Ada\n",
    "training_config = {\n",
    "    'batch_size': 32,  # Updated from 16\n",
    "    'accumulation_steps': 2,  # Effective batch size = 64, as before\n",
    "    'mixed_precision': True,\n",
    "    'gradient_checkpointing': True,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': 4,\n",
    "    'max_image_size': 224,\n",
    "}\n",
    "\n",
    "# Scaler for mixed precision\n",
    "scaler = GradScaler(device='cuda')\n",
    "\n",
    "def print_memory_stats(step_tag=\"\"):\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"[{step_tag}] GPU Memory: Allocated={allocated:.2f}MB | Reserved={reserved:.2f}MB\")\n",
    "\n",
    "# Simulated loss function (replace with real one)\n",
    "def compute_loss(outputs, targets):\n",
    "    return nn.functional.mse_loss(outputs, targets)\n",
    "\n",
    "# Example training step\n",
    "def train_step(model, batch, optimizer, step):\n",
    "    model.train()\n",
    "\n",
    "    if training_config['mixed_precision']:\n",
    "        print(f\"[Step {step}] Mixed precision enabled ✔\")\n",
    "\n",
    "    # Print memory before forward\n",
    "    print_memory_stats(f\"Step {step} - Before Forward\")\n",
    "\n",
    "    with autocast(dtype=torch.float16):\n",
    "        outputs = model(batch['images'], batch['texts'])\n",
    "        loss = compute_loss(outputs, batch['targets'])\n",
    "        loss = loss / training_config['accumulation_steps']\n",
    "        print(f\"[Step {step}] Scaled Loss: {loss.item():.6f}\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Print memory after backward\n",
    "    print_memory_stats(f\"Step {step} - After Backward\")\n",
    "\n",
    "    if (step + 1) % training_config['accumulation_steps'] == 0:\n",
    "        print(f\"[Step {step}] Performing optimizer step...\")\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Force garbage collection (optional but useful for debugging memory)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print_memory_stats(f\"Step {step} - After Optimizer Step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1429897d-4c22-4449-bb98-cc153b41b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efficient Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50de4fb1-a1fc-4f8c-830b-5ec3d19869dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Memory-efficient transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Start small\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Optimized DataLoader for your system\n",
    "def create_dataloader(dataset, batch_size=16):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Good for your CPU\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        persistent_workers=True,  # Reuse workers\n",
    "        prefetch_factor=2  # Prefetch batches\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a648cd35-4459-46bb-bc3c-f2994dcaf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auto-pick or manually assign GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3127ccbd-00da-45a2-868e-8a70424810e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Auto-selected GPU: 0\n",
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from utils.gpu_manager import GPUMemoryManager\n",
    "\n",
    "# Auto-pick or manually assign GPU (e.g., 0 = RTX 2000 Ada, 1 = RTX 6000)\n",
    "gpu_manager = GPUMemoryManager(preferred_device=None)\n",
    "device = gpu_manager.get_device()\n",
    "\n",
    "print(f\"Running on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b4bb3-4f54-4f0e-8d22-cebc85e568e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11300796-2519-46c1-9a76-237235fabb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_name': 'Lightweight Cross-Modal Attention', 'preferred_device': None, 'seed': 42, 'datasets_path': './datasets/', 'coco_annotations': 'coco_subset/annotations/captions_train2017.json', 'coco_val_annotations': 'coco_subset/annotations/captions_val2017.json', 'vqa_annotations': 'vqa2/v2_mscoco_train2014_annotations.json', 'vqa_questions': 'vqa2/v2_OpenEnded_mscoco_train2014_questions.json', 'nocaps_json': 'nocaps/nocaps_test_public.json', 'flickr_csv': 'flickr30k_images_andCaptions/flickr30k_captions.csv', 'rank_k': 32, 'input_dim': 512, 'epochs': 10, 'learning_rate': 5e-05, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('./configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b848e53a-7d78-4575-9f11-e863da23018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loader in your Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281dab6a-3561-4c18-8aac-fa67ae295cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 414113 valid COCO samples, skipped 177640 missing images.\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Tokenized caption: tensor([ 101, 1037, 2200, 4550, 1998, 2092, 7429, 4064, 5723,  102,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from data_loader import CrossModalDatasetLoader\n",
    "\n",
    "# Load config\n",
    "with open('./configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Instantiate loader\n",
    "loader = CrossModalDatasetLoader(config)\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = loader.load_coco()\n",
    "\n",
    "# Test one sample\n",
    "sample = coco_dataset[0]\n",
    "\n",
    "print(\"Image shape:\", sample['image'].shape)\n",
    "print(\"Tokenized caption:\", sample['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7063b74-73cb-481a-a826-02ece1cbb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Sanity Test before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1aeeb34-8003-41ea-b1df-a44f29517943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Auto-selected GPU: 0\n",
      "Loaded 414113 valid samples, skipped 177640 missing images.\n",
      "Output score: tensor([-0.1207], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from data_loader import CrossModalDatasetLoader\n",
    "from models.multimodal_model import CrossModalModel\n",
    "from utils.gpu_manager import GPUMemoryManager\n",
    "\n",
    "# Load config\n",
    "with open('./configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup device\n",
    "gpu_manager = GPUMemoryManager(preferred_device=None)\n",
    "device = gpu_manager.get_device()\n",
    "\n",
    "# Load dataset\n",
    "loader = CrossModalDatasetLoader(config)\n",
    "dataset = loader.load_coco(split=\"train\")\n",
    "\n",
    "# Take a small batch (simulate dataloader)\n",
    "sample = dataset[0]\n",
    "\n",
    "# Build batch manually for test\n",
    "batch = {\n",
    "    'image': sample['image'].unsqueeze(0),        # add batch dimension (1, 3, 224, 224)\n",
    "    'input_ids': sample['input_ids'].unsqueeze(0) # (1, 50)\n",
    "}\n",
    "\n",
    "# Load model\n",
    "model = CrossModalModel(device=device, rank_k=config['rank_k']).to(device)\n",
    "\n",
    "# Forward pass\n",
    "scores = model(batch)\n",
    "\n",
    "print(\"Output score:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43353e64-28d2-4136-9d4d-f531d10edc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01e8220-76e1-495c-a6b1-09ae3da2ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Auto-selected GPU: 0\n",
      "Loaded 414113 valid samples, skipped 177640 missing images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] finished. Average Loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10] finished. Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import yaml\n",
    "from data_loader import CrossModalDatasetLoader\n",
    "from models.multimodal_model import CrossModalModel\n",
    "from trainer.train import Trainer\n",
    "from utils.gpu_manager import GPUMemoryManager\n",
    "\n",
    "# Load config\n",
    "with open('./configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# GPU manager\n",
    "gpu_manager = GPUMemoryManager(preferred_device=None)\n",
    "device = gpu_manager.get_device()\n",
    "\n",
    "# Dataset\n",
    "loader = CrossModalDatasetLoader(config)\n",
    "dataset = loader.load_coco(split=\"train\")\n",
    "\n",
    "# Model\n",
    "model = CrossModalModel(device=device, rank_k=config['rank_k']).to(device)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model, dataset, config, device)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf0160-5c6f-49ad-956c-6dceb361b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluvation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29849b2-486e-4065-b171-0781fc73f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Evaluation (COCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a544ad37-9c42-4529-a7b0-4417ac41325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded 25014 valid samples, skipped 0 missing images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding dataset: 100%|██████████████████| 25014/25014 [03:50<00:00, 108.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall@1': 3.997761253697929e-05, 'Recall@5': 0.00019988806268489645, 'Recall@10': 0.0003997761253697929}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import yaml\n",
    "from data_loader import CrossModalDatasetLoader\n",
    "from models.multimodal_model import CrossModalModel\n",
    "from evaluation.retrieval_evaluator import RetrievalEvaluator\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Cell 2: Load Config\n",
    "with open('./configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Cell 3: Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Cell 4: Load Dataset\n",
    "loader = CrossModalDatasetLoader(config)\n",
    "coco_dataset = loader.load_coco(split=\"val\")\n",
    "\n",
    "# Cell 5: Load Model (Make sure you trained your model first)\n",
    "model = CrossModalModel(device=device, rank_k=config['rank_k']).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Cell 6: Run Retrieval Evaluation\n",
    "retrieval_eval = RetrievalEvaluator(model, coco_dataset, device, save_dir=\"./results/retrieval/coco\")\n",
    "retrieval_eval.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b39969-6658-47e3-bd2a-998d40e5d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.multimodal_model import CrossModalModel\n",
    "\n",
    "# Load config again\n",
    "import yaml\n",
    "with open('./configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Rebuild model\n",
    "model = CrossModalModel(device=device, rank_k=config['rank_k']).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f4a28-0987-47ca-a8c2-7cbefd7f966b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flickr30K Columns: Index(['image_file', 'caption'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding dataset: 100%|████████████████| 155070/155070 [21:05<00:00, 122.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Flickr30K dataset\n",
    "flickr_dataset = loader.load_flickr30k()\n",
    "\n",
    "# Import evaluator (only if not already imported)\n",
    "from evaluation.retrieval_evaluator import RetrievalEvaluator\n",
    "\n",
    "# Run retrieval evaluation for Flickr30K\n",
    "retrieval_eval = RetrievalEvaluator(model, flickr_dataset, device, save_dir=\"./results/retrieval/flickr\")\n",
    "retrieval_eval.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43956f32-711d-4c17-ae04-adbcf594fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captioning Evaluation (NoCaps)\n",
    "# Generated captions JSON: \"./results/generated_captions.json\"\n",
    "# Ground truth JSON: \"./datasets/nocaps/ground_truth.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62e8524-d5fd-4f57-b59c-9c82cf7edd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Sample Keys: ['179765', '190236', '331352', '517069', '182417']\n",
      "Generated Captions Sample Keys: ['COCO_val2017_000000179765.jpg', 'COCO_val2017_000000190236.jpg', 'COCO_val2017_000000331352.jpg', 'COCO_val2017_000000517069.jpg', 'COCO_val2017_000000182417.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load both JSONs\n",
    "with open(\"./datasets/nocaps/ground_truth_coco_val2017.json\") as f:\n",
    "    gt_data = json.load(f)\n",
    "with open(\"./results/generated_captions_coco_val2017.json\") as f:\n",
    "    gen_data = json.load(f)\n",
    "\n",
    "# Sample 5 keys from both sides\n",
    "print(\"Ground Truth Sample Keys:\", list(gt_data.keys())[:5])\n",
    "print(\"Generated Captions Sample Keys:\", list(gen_data.keys())[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4787cfbf-e44a-4cb2-9e5f-205698d48017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ground truth file created at: ./datasets/nocaps/ground_truth_coco_val2017.json\n",
      "✅ Auto-selected GPU: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions: 100%|██████████████████| 5000/5000 [02:38<00:00, 31.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated captions saved at ./results/generated_captions_coco_val2017.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/loom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/loom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/loom/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated captions: 5000\n",
      "Total ground truth captions: 5000\n",
      "Common samples found for evaluation: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Samples: 100%|██████████████████| 5000/5000 [00:06<00:00, 791.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 1.0, 'METEOR': 0.9995758906089781, 'CIDEr': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# (1) Generate ground truth (already done)\n",
    "%run ./utils/generate_coco_val2017_groundtruth.py\n",
    "\n",
    "# (2) Generate captions (new updated code above)\n",
    "%run ./inference/caption_generator_coco_val2017.py\n",
    "\n",
    "from evaluation.captioning_evaluator import CaptioningEvaluator\n",
    "import os\n",
    "\n",
    "# Disable tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Provide the paths\n",
    "caption_eval = CaptioningEvaluator(\n",
    "    ground_truth_path=\"./datasets/nocaps/ground_truth_coco_val2017.json\",\n",
    "    generated_captions_path=\"./results/generated_captions_coco_val2017.json\",\n",
    "    save_dir=\"./results/captioning/\"\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "caption_eval.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9653ea-9ac7-4172-9c18-903e5a9f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VQA Evaluation (VQAv2 or OK-VQA)\n",
    "#Assuming you already have:\n",
    "#Ground truth: \"./datasets/vqa2/ground_truth.json\"\n",
    "#Model predictions: \"./results/vqa_predictions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36003491-bbaf-4ac1-9474-d55a788e7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.vqa_evaluator import VQAEvaluator\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Create evaluator\n",
    "vqa_eval = VQAEvaluator(\n",
    "    ground_truth_path=\"./datasets/vqa2/ground_truth.json\",\n",
    "    predictions_path=\"./results/vqa_predictions.json\",\n",
    "    save_dir=\"./results/vqa/\"\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "vqa_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b49c3f-f58f-4c97-8331-c0c74bc565d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efficiency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047545a-f263-4c29-beb8-002d714a9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.efficiency_evaluator import EfficiencyEvaluator\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Create evaluator\n",
    "eff_eval = EfficiencyEvaluator(model, save_dir=\"./results/efficiency/\")\n",
    "\n",
    "# Run evaluation\n",
    "eff_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72117148-a370-4eb8-9daa-d0fbb36ca8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Full Ablation Study\n",
    "#This will automatically launch 5 full experiments back-to-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98d100-727e-4e7e-9af7-53e39080ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automation.ablation_runner import AblationRunner\n",
    "\n",
    "runner = AblationRunner()\n",
    "runner.run_ablation(rank_list=[16, 32, 64, 128, 256])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
