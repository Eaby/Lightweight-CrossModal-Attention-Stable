project_name: Lightweight Cross-Modal Attention

# Device management
preferred_device: null    # null means auto-select, 'cuda' or 'cpu' can be set explicitly
seed: 42

# Global paths
datasets_path: "./datasets"
results_dir: "./results" # Main directory for all model outputs, evaluation results, and checkpoints

# Datasets configurations
# COCO Captions (Primary for Retrieval and Captioning)
coco_annotations: "coco_subset/annotations/captions_train2017.json"
coco_val_annotations: "coco_subset/annotations/captions_val2017.json"
train_image_dir: "coco_subset/train2017" # Path to COCO 2017 training images (e.g., train2017 folder)
val_image_dir: "coco_subset/val2017"   # Path to COCO 2017 validation images (e.g., val2017 folder)

# Flickr30K (Secondary for Image-Text Retrieval)
flickr_csv: "flickr30k_images_andCaptions/flickr30k_captions.csv"
# Note: Flickr30K images are assumed to be in datasets_path/flickr30k_images_andCaptions/

# VQAv2 (Primary for Visual Question Answering) - uses COCO 2014 images
vqa_train_questions: "vqa2/v2_OpenEnded_mscoco_train2014_questions.json"
vqa_train_annotations: "vqa2/v2_mscoco_train2014_annotations.json"
vqa_train_image_dir: "coco_subset/train2014" # VQA uses COCO 2014 train images

vqa_val_questions: "vqa2/v2_OpenEnded_mscoco_val2014_questions.json"
vqa_val_annotations: "vqa2/v2_mscoco_val2014_annotations.json"
vqa_val_image_dir: "coco_subset/val2014"   # VQA uses COCO 2014 val images

# NoCaps (for Image Captioning Evaluation - Out-of-domain)
nocaps_annotations: "nocaps/nocaps_test_public.json" # Path to NoCaps annotation file
nocaps_image_dir: "nocaps/images"                     # Path to NoCaps image folder (e.g., where actual images are)

# OK-VQA (for Visual Question Answering Evaluation - Knowledge-based) - uses COCO 2014 images
okvqa_train_questions: "OK-VQA/OpenEnded_mscoco_train2014_questions.json" # Assuming these are unzipped
okvqa_train_annotations: "OK-VQA/mscoco_train2014_annotations.json"     # Assuming these are unzipped
okvqa_val_questions: "OK-VQA/OpenEnded_mscoco_val2014_questions.json"   # Assuming these are unzipped
okvqa_val_annotations: "OK-VQA/mscoco_val2014_annotations.json"       # Assuming these are unzipped

# Model hyperparameters
rank_k: 32           
input_dim: 768       
max_text_length: 77  
attention_temperature: 0.07 

# Task-specific output dimensions (for model heads)
vocab_size: 30522    
num_answers: 3172    

# NEW: Transformer Decoder hyperparameters for Captioning
decoder_nheads: 8
decoder_dim_feedforward: 2048
decoder_dropout: 0.1
decoder_num_layers: 2 # Number of decoder layers for captioning

# Training hyperparameters
epochs: 10
batch_size: 128       # Training batch size

# Learning Rates (differentiated as per plan for multi-task training)
lr_attention: 0.0001 # Learning rate for the custom RankBasedCrossModalAttention module
lr_heads: 0.0005     # Learning rate for task-specific prediction heads (retrieval, captioning, VQA)
# learning_rate: 0.00005 # Legacy/deprecated: Consider removing if lr_attention/lr_heads always used
weight_decay: 0.01   # L2 regularization for optimizer
warmup_steps: 1000   # Number of warmup steps for learning rate scheduler (e.g., cosine with warmup)

# Multi-task weighting (for balancing loss contributions during training)
task_weights:
  retrieval: 1.0     # Weight for image-text retrieval loss
  captioning: 0.5    # Weight for image captioning loss
  vqa: 0.8           # Weight for visual question answering loss

# Evaluation and Checkpointing Frequencies (in training steps)
evaluation_frequency_steps: 5000 # How often to run evaluation during training (e.g., every 500 steps)
save_frequency_steps: 1000      # How often to save model checkpoints during training

# Evaluation specific parameters
eval_batch_size: 256  # Batch size to use during evaluation (can be larger than training batch size)
eval_num_workers: 4  # Number of data loading workers for evaluation
vqa_predictions_filename: "vqa_predictions.json" # Default filename for VQA predictions generated by inference
caption_predictions_filename: "generated_captions.json" # Default filename for captions generated by inference

# NEW: Checkpoint Resume/Save Configuration
resume_from_checkpoint: null # Set to "path/to/checkpoint.pth" to resume, or null for new training
checkpoint_dir: "./results/checkpoints" # Directory where checkpoints will be saved
final_model_dir: "./results/final_model" # Directory for the final trained model
best_model_dir: "./results/best_model" # Directory for the best model based on primary metric

# --- Added to fix attribute errors ---
vqa_questions: "vqa2/v2_OpenEnded_mscoco_train2014_questions.json"       # Added for dataset loader
vqa_annotations: "vqa2/v2_mscoco_train2014_annotations.json"            # Added for dataset loader


okvqa_train_questions: "OK-VQA/OpenEnded_mscoco_train2014_questions.json.zip"
okvqa_train_annotations: "OK-VQA/mscoco_train2014_annotations.json.zip"
okvqa_val_questions: "OK-VQA/OpenEnded_mscoco_val2014_questions.json.zip"
okvqa_val_annotations: "OK-VQA/mscoco_val2014_annotations.json.zip"
