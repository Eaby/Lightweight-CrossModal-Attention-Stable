{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35cdf34-7b7b-47ea-aab5-4d86b269f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/loom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/loom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/loom/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU cache cleared.\n",
      "Using device: 0\n",
      "Loaded 591753 valid COCO samples, skipped 0 missing images.\n",
      "Loaded 25014 valid COCO samples, skipped 0 missing images.\n",
      "Train dataset size: 591753\n",
      "Validation dataset size: 25014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/AAAI/Lightweight Cross-Modal Attention/trainer/train.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "/data/AAAI/Lightweight Cross-Modal Attention/trainer/train.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] finished. Average Loss: 0.693152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] finished. Average Loss: 0.693148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9] finished. Average Loss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10] finished. Average Loss: 0.693147\n",
      "✅ Model successfully saved after training.\n",
      "✅ Model saved at ./results/final_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'predictions_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Run Captioning Evaluation\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m caption_eval \u001b[38;5;241m=\u001b[39m \u001b[43mCaptioningEvaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mground_truth_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnocaps/ground_truth_coco_val2017.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/generated_captions_coco_val2017.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/captioning/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m caption_eval\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Run VQA Evaluation\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'predictions_path'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader import CrossModalDatasetLoader\n",
    "from models.multimodal_model import CrossModalModel\n",
    "from trainer.train import Trainer\n",
    "from evaluation.captioning_evaluator import CaptioningEvaluator\n",
    "from evaluation.vqa_evaluator import VQAEvaluator\n",
    "from evaluation.efficiency_evaluator import EfficiencyEvaluator\n",
    "from utils.config_loader import load_config\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"✅ GPU cache cleared.\")\n",
    "\n",
    "# Load config yaml (adjust path if needed)\n",
    "config = load_config('./configs/config.yaml')\n",
    "print(f\"Using device: {torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load datasets\n",
    "dataset_loader = CrossModalDatasetLoader(config)\n",
    "train_dataset = dataset_loader.load_coco(split=\"train\")\n",
    "val_dataset = dataset_loader.load_coco(split=\"val\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize model and trainer\n",
    "model = CrossModalModel(device=device, rank_k=config['rank_k']).to(device)\n",
    "trainer = Trainer(model=model, dataset=train_dataset, config=config, device=device)\n",
    "\n",
    "# Run training\n",
    "trainer.train()\n",
    "\n",
    "# Save final model checkpoint\n",
    "save_path = \"./results/final_model.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"✅ Model saved at {save_path}\")\n",
    "\n",
    "# Run Captioning Evaluation\n",
    "caption_eval = CaptioningEvaluator(\n",
    "    ground_truth_path=os.path.join(config['datasets_path'], \"nocaps/ground_truth_coco_val2017.json\"),\n",
    "    predictions_path=\"./results/generated_captions_coco_val2017.json\",\n",
    "    save_dir=\"./results/captioning/\"\n",
    ")\n",
    "caption_eval.evaluate()\n",
    "\n",
    "# Run VQA Evaluation\n",
    "vqa_eval = VQAEvaluator(\n",
    "    ground_truth_path=os.path.join(config['datasets_path'], \"vqa2/ground_truth.json\"),\n",
    "    predictions_path=\"./results/vqa_predictions.json\",\n",
    "    save_dir=\"./results/vqa/\"\n",
    ")\n",
    "vqa_eval.evaluate()\n",
    "\n",
    "# Run Efficiency Evaluation\n",
    "efficiency_eval = EfficiencyEvaluator(model, save_dir=\"./results/efficiency/\")\n",
    "efficiency_eval.evaluate()\n",
    "\n",
    "# Example: Plot training loss curve if Trainer records it (assuming trainer.losses list)\n",
    "if hasattr(trainer, 'losses'):\n",
    "    plt.plot(trainer.losses)\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.xlabel(\"Batch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223122a9-82fc-44b2-94cc-5c2738738a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and Data Setup\n",
    "\n",
    "# Load config yaml or set manually\n",
    "import yaml\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate dataset and model\n",
    "train_dataset = CustomDataset(config['dataset']['train_path'])\n",
    "model = MultimodalModel(config['model']).to(device)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model=model, dataset=train_dataset, config=config['training'], device=device)\n",
    "\n",
    "print(\"✅ Config and dataset/model setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa4eaa-2429-4573-869f-2da3a872701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run Training and Save Model\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Model saved inside trainer.train() - confirm path or save again if needed\n",
    "print(\"✅ Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb0bd6-fe87-45d8-bcc1-f1a638ac4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run Captioning Evaluation\n",
    "\n",
    "caption_eval = CaptioningEvaluator(\n",
    "    ground_truth_path=\"./datasets/nocaps/ground_truth_coco_val2017.json\",\n",
    "    generated_captions_path=\"./results/generated_captions_coco_val2017.json\",\n",
    "    save_dir=\"./results/captioning/\"\n",
    ")\n",
    "caption_eval.evaluate()\n",
    "print(\"✅ Captioning evaluation done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cfd87-e671-437d-9929-78be5046a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run VQA Evaluation\n",
    "\n",
    "vqa_eval = VQAEvaluator(\n",
    "    ground_truth_path=\"./datasets/vqa2/ground_truth.json\",\n",
    "    predictions_path=\"./results/vqa_predictions.json\",\n",
    "    save_dir=\"./results/vqa/\"\n",
    ")\n",
    "vqa_eval.evaluate()\n",
    "print(\"✅ VQA evaluation done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a1331-a6f0-4583-aaf4-79fa5e6a2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Efficiency Evaluation\n",
    "\n",
    "eff_eval = EfficiencyEvaluator(model=model, save_dir=\"./results/efficiency/\")\n",
    "eff_eval.evaluate()\n",
    "print(\"✅ Efficiency evaluation done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a361e-be1d-473a-81d7-08b08f73b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run Ablation Study (optional, long runtime)\n",
    "\n",
    "runner = AblationRunner()\n",
    "runner.run_ablation(rank_list=[16, 32, 64, 128, 256])\n",
    "print(\"✅ Ablation studies completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d576a-adcd-4395-b550-1824430fa0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize and Summarize Results (you can expand this with matplotlib/seaborn)\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_json_results(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "caption_results = load_json_results('./results/captioning/captioning_results.json')\n",
    "vqa_results = load_json_results('./results/vqa/vqa_results.json')\n",
    "efficiency_results = load_json_results('./results/efficiency/efficiency_results.json')\n",
    "\n",
    "print(\"Captioning Results:\", caption_results)\n",
    "print(\"VQA Results:\", vqa_results)\n",
    "print(\"Efficiency Results:\", efficiency_results)\n",
    "\n",
    "# Example plotting BLEU from captioning results\n",
    "plt.bar(caption_results.keys(), caption_results.values())\n",
    "plt.title(\"Captioning Evaluation Metrics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87479e-726b-4703-aaf2-ed159335f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final summary\n",
    "\n",
    "print(\"\"\"\n",
    "Project Evaluation Summary:\n",
    "\n",
    "- Training completed and model saved.\n",
    "- Captioning, VQA, efficiency, and ablation evaluations executed.\n",
    "- Results saved to ./results/ folders.\n",
    "- Summary table and plots generated.\n",
    "\n",
    "You can export this notebook as HTML or PDF to include in your paper draft.\n",
    "\n",
    "Next Steps:\n",
    "- Analyze ablation study CSV/log files for detailed insights.\n",
    "- Customize plots for paper figures.\n",
    "- Prepare manuscript with methodology and results sections.\n",
    "\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
